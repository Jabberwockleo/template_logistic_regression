{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph config\n",
    "FEATURE_NUM = 4\n",
    "\n",
    "# Training config\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHES = 10\n",
    "BATCH_SIZE = 1\n",
    "DISPLAY_STEP = 100\n",
    "\n",
    "# output dir\n",
    "CHKPT_DIR = '/tmp/chkpt/lr'\n",
    "LOG_DIR = '/tmp/log/lr'\n",
    "MODEL_DIR = '/tmp/model/lr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "\n",
    "# iris data\n",
    "from sklearn import datasets\n",
    "dataset_ori = datasets.load_iris(return_X_y=True)\n",
    "y_label = map(lambda x: x == 0, dataset_ori[1])\n",
    "dataset = []\n",
    "dataset.append(dataset_ori[0])\n",
    "dataset.append(np.array(list(y_label)).astype(int))\n",
    "\n",
    "# mock data\n",
    "#def mock_boundary_func(X):\n",
    "#    # mock weights is (1, 2, 3, ..)\n",
    "#    return np.sum(np.dot(X, list(range(1, len(X) + 1))))\n",
    "#dataset = []\n",
    "#dataset.append(np.random.standard_normal(size=(1000, FEATURE_NUM)))\n",
    "#dataset.append(np.array(list(map(lambda x: mock_boundary_func(x) >= 0, dataset[0]))).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch util\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5.1, 3.5, 1.4, 0.2]), array([4.9, 3. , 1.4, 0.2]), array([4.7, 3.2, 1.3, 0.2])) (1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# test batch\n",
    "for tu in batch(list(zip(dataset[0], dataset[1])), n=3):\n",
    "    X, y = zip(*tu)\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(object):\n",
    "    \"\"\"\n",
    "        Logistic Regressor\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_num, learning_rate=1e-2, random_seed=None):\n",
    "        \"\"\"\n",
    "            Initializer\n",
    "            Params:\n",
    "                feature_num: feature number\n",
    "                learning_rate: learning rate\n",
    "                random_seed: random seed\n",
    "        \"\"\"\n",
    "        self.feature_num = feature_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_seed = random_seed\n",
    "        self.construct_placeholders()\n",
    "        \n",
    "    def construct_placeholders(self):\n",
    "        \"\"\"\n",
    "            Construct inpute placeholders\n",
    "        \"\"\"\n",
    "        self.input_feature_vectors = tf.placeholder(shape=[None, self.feature_num],\n",
    "            dtype=tf.float32)\n",
    "        self.input_labels = tf.placeholder(shape=[None, 1],\n",
    "            dtype=tf.float32)\n",
    "    \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "            Build graph\n",
    "        \"\"\"\n",
    "        self.construct_weights()\n",
    "        \n",
    "        # network forward pass\n",
    "        saver, logits = self.forward_pass()\n",
    "        \n",
    "        # loss function\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.input_labels))\n",
    "        \n",
    "        # training optimizer\n",
    "        train_op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n",
    "        \n",
    "        # statistics\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        stat_merged = tf.summary.merge_all()\n",
    "\n",
    "        return saver, logits, loss, train_op, stat_merged\n",
    "    \n",
    "    def construct_weights(self):\n",
    "        \"\"\"\n",
    "            Construct weights\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(1):\n",
    "            weight_key = \"w_{}_{}\".format(i, i+1)\n",
    "            bias_key = \"b_{}\".format(i)\n",
    "            self.weights.append(tf.get_variable(\n",
    "                name=weight_key, shape=[self.feature_num, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases.append(tf.get_variable(\n",
    "                name=bias_key, shape=[1],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # statistics\n",
    "            tf.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases[-1])\n",
    "            \n",
    "    def forward_pass(self):\n",
    "        \"\"\"\n",
    "            Forward pass\n",
    "        \"\"\"\n",
    "        h = self.input_feature_vectors\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.sigmoid(h)\n",
    "\n",
    "        return tf.train.Saver(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "        Sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calc_accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "        Calc accuracy\n",
    "    \"\"\"\n",
    "    pred_labels = np.round(sigmoid(logits))\n",
    "    match_score = np.equal(pred_labels, labels).astype(np.float32)\n",
    "    return np.mean(match_score)\n",
    "\n",
    "def calc_f1(logits, labels, log_confusion_matrix=False):\n",
    "    \"\"\"\n",
    "        Calc F1 score\n",
    "    \"\"\"\n",
    "    pred_labels = np.round(sigmoid(np.array(logits))).ravel()\n",
    "    real_labels = np.array(labels).ravel()\n",
    "    ind_1 = np.argwhere(real_labels == 1)\n",
    "    ind_0 = np.argwhere(real_labels == 0)\n",
    "    tp = np.sum(pred_labels[ind_1])\n",
    "    tn = np.sum((1 - pred_labels)[ind_0])\n",
    "    fp = np.sum(pred_labels[ind_0])\n",
    "    fn = np.sum((1 - pred_labels)[ind_1])\n",
    "    f1 = 2.0 * tp / (2*tp + fn + fp)\n",
    "    acc = (tp + tn) * 1.0 / (tp + tn + fp + fn)\n",
    "    if log_confusion_matrix is True:\n",
    "        print(\"tp:{}, tn:{}, fp:{}, fn:{}, acc:{}, len:{}\".format(\n",
    "        tp, tn, fp, fn, acc, len(logits)))\n",
    "    return f1\n",
    "\n",
    "def calc_auc(logits, labels):\n",
    "    \"\"\"\n",
    "        Calc AUC\n",
    "    \"\"\"\n",
    "    return roc_auc_score(labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp:2.0, tn:1.0, fp:1.0, fn:0.0, acc:0.75, len:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 1, 1, 0]\n",
    "b = [0, 1, 1, 0]\n",
    "calc_f1(a, b, log_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "tf.reset_default_graph()\n",
    "lr = LogisticRegressor(feature_num=FEATURE_NUM, learning_rate=LEARNING_RATE, random_seed=None)\n",
    "saver, logits, loss, train_op, stat_merged = lr.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, global_step: 100, loss: 0.8411799073219299, accuracy: 0.6666666865348816, f1: 0.0, auc: 1.0\n",
      "epoch: 1, global_step: 200, loss: 0.5090888142585754, accuracy: 0.6666666865348816, f1: 0.0, auc: 1.0\n",
      "epoch: 1, global_step: 300, loss: 0.31451234221458435, accuracy: 0.6933333277702332, f1: 0.14814814814814814, auc: 1.0\n",
      "epoch: 2, global_step: 400, loss: 0.24150700867176056, accuracy: 0.95333331823349, f1: 0.9247311827956989, auc: 1.0\n",
      "epoch: 3, global_step: 500, loss: 0.2205905169248581, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 3, global_step: 600, loss: 0.20642343163490295, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 4, global_step: 700, loss: 0.1977553367614746, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 5, global_step: 800, loss: 0.1915125548839569, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 5, global_step: 900, loss: 0.18618500232696533, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 6, global_step: 1000, loss: 0.1811148226261139, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 7, global_step: 1100, loss: 0.1757250726222992, accuracy: 1.0, f1: 1.0, auc: 1.0\n",
      "epoch: 7, global_step: 1200, loss: 0.17095214128494263, accuracy: 1.0, f1: 1.0, auc: 1.0\n",
      "epoch: 8, global_step: 1300, loss: 0.16658908128738403, accuracy: 0.9933333396911621, f1: 0.98989898989899, auc: 1.0\n",
      "epoch: 9, global_step: 1400, loss: 0.16221466660499573, accuracy: 1.0, f1: 1.0, auc: 1.0\n",
      "epoch: 9, global_step: 1500, loss: 0.15796080231666565, accuracy: 1.0, f1: 1.0, auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # log dir\n",
    "    log_dir = LOG_DIR\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())\n",
    "    \n",
    "    # checkpoint dir\n",
    "    chkpt_dir = CHKPT_DIR\n",
    "    if os.path.exists(chkpt_dir):\n",
    "        shutil.rmtree(chkpt_dir)\n",
    "    if not os.path.isdir(chkpt_dir):\n",
    "        os.makedirs(chkpt_dir)\n",
    "    \n",
    "    for epoch in range(EPOCHES):\n",
    "        batch_cnt = 0\n",
    "        batches_per_epoch = math.floor((len(dataset[0]) - 1) * 1.0 / BATCH_SIZE) + 1\n",
    "        best_loss = np.inf\n",
    "        cur_loss = np.inf\n",
    "        cur_accuracy = 0\n",
    "        training_data = list(zip(dataset[0], dataset[1]))\n",
    "        random.shuffle(training_data)\n",
    "        for tu in batch(training_data, n=BATCH_SIZE):\n",
    "            X, y = zip(*tu)\n",
    "            y = np.expand_dims(y, 1)\n",
    "            feed_dict = {\n",
    "                lr.input_feature_vectors: X,\n",
    "                lr.input_labels: y\n",
    "            }\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            batch_cnt += 1\n",
    "            global_step = epoch * batches_per_epoch + batch_cnt\n",
    "            if global_step % DISPLAY_STEP == 0:\n",
    "                in_f = dataset[0]\n",
    "                in_l = np.expand_dims(dataset[1], 1)\n",
    "                feed_dict = {\n",
    "                    lr.input_feature_vectors: in_f,\n",
    "                    lr.input_labels: in_l\n",
    "                }\n",
    "                cur_loss, cur_logits = sess.run([loss, logits], feed_dict=feed_dict)\n",
    "                summary_train = sess.run(stat_merged, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=global_step)\n",
    "                print(\"epoch: {}, global_step: {}, loss: {}, \"\n",
    "                    \"accuracy: {}, f1: {}, auc: {}\".format(\n",
    "                    epoch, global_step, cur_loss,\n",
    "                    calc_accuracy(cur_logits, in_l),\n",
    "                    calc_f1(cur_logits, in_l),\n",
    "                    calc_auc(cur_logits, in_l)))\n",
    "        if cur_loss < best_loss:\n",
    "            best_loss = cur_loss\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "# tensorboard\n",
    "# tensorboard --logdir /tmp/log/lr --port 8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/chkpt/lr/model\n",
      "tp:50.0, tn:50.0, fp:0.0, fn:0.0, acc:1.0, len:100\n",
      "accuracy: 1.0, f1: 1.0, auc: 1.0\n",
      "weights:  [array([[-0.43972158],\n",
      "       [ 1.1392181 ],\n",
      "       [-0.25990582],\n",
      "       [-1.1183376 ]], dtype=float32)]\n",
      "biases:  [array([0.13066643], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# predict using checkpoint\n",
    "tf.reset_default_graph()\n",
    "lr = LogisticRegressor(feature_num=FEATURE_NUM, learning_rate=LEARNING_RATE, random_seed=None)\n",
    "saver, logits, loss, train_op, stat_merged = lr.build_graph()\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "    \n",
    "    in_feature_vecs = dataset[0][:100]\n",
    "    in_labels = dataset[1][:100]\n",
    "    in_labels = np.expand_dims(in_labels, 1)\n",
    "    feed_dict = {\n",
    "        lr.input_feature_vectors: in_feature_vecs,\n",
    "        lr.input_labels: in_labels\n",
    "    }\n",
    "    out_logits, out_weights, out_biases = sess.run(\n",
    "        [logits, lr.weights, lr.biases], feed_dict=feed_dict)\n",
    "    print(\"accuracy: {}, f1: {}, auc: {}\".format(\n",
    "        calc_accuracy(out_logits, in_labels),\n",
    "        calc_f1(out_logits, in_labels, log_confusion_matrix=True),\n",
    "        calc_auc(out_logits, in_labels)))\n",
    "    print(\"weights: \", out_weights)\n",
    "    print(\"biases: \", out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# tensorboard --logdir /tmp/log/lr --port 8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
